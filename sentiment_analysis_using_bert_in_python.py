# -*- coding: utf-8 -*-
"""Sentiment_Analysis_using_BERT_in_Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x4Oy6WrwKRagzq0oY6F_n_m0_V223cNQ
"""

import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from torch import nn
from torch.utils.data import DataLoader

from transformers import BertModel, BertTokenizerFast, get_linear_schedule_with_warmup, DataCollatorWithPadding
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score
from tqdm import tqdm, trange
from torch.optim import AdamW

from datasets import load_dataset, Dataset, DatasetDict

from dataclasses import dataclass
@dataclass
class Config:
    model_name: str = "bert-base-uncased"
    max_length: int = 256
    train_batch_size: int = 12
    eval_batch_size: int = 32
    gradient_accumulation_steps: int = 1
    epochs: int = 3
    learning_rate: float = 2e-5
    weight_decay: float = 0.01
    warmup_steps: int = 0
    seed: int = 42
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    output_dir: str = "./saved_model"
    num_labels: int = 2  # IMDb is binary (pos/neg)
    logging_steps: int = 50
    max_grad_norm: float = 1.0

cfg = Config()

def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

set_seed(cfg.seed)

os.makedirs(cfg.output_dir, exist_ok=True)

raw = load_dataset('imdb')

raw['train']

# tokenizer
tokenizer = BertTokenizerFast.from_pretrained(cfg.model_name)

def preprocess(examples):
    texts = examples['text']
    tokenized_texts = tokenizer(texts, padding='max_length', max_length=cfg.max_length, truncation=True)
    return tokenized_texts

tokenized_datasets = raw.map(preprocess, batched=True, remove_columns=['text'])

tokenized_datasets['train']

data_collector = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')

train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=cfg.train_batch_size, shuffle=True, collate_fn=data_collector)
eval_dataloader = DataLoader(tokenized_datasets['test'], batch_size=cfg.eval_batch_size, shuffle=False, collate_fn=data_collector)

# classificaton model
class BertForSentimentAnalysis(nn.Module):
    def __init__(self, model_name: str, num_labels: int = 2, dropout_prob: float = 0.1):
        super().__init__()
        self.bert = BertModel.from_pretrained(model_name)
        hidden_size = self.bert.config.hidden_size  # typically 768 for base
        # A simple classification head
        self.classifier = nn.Sequential(
            nn.Dropout(p=dropout_prob),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(p=dropout_prob / 2),
            nn.Linear(hidden_size // 2, num_labels),
        )

    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):
        # Get last hidden states from BERT (we use pooled output / CLS token representation)
        bert_outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            return_dict=True,
        )
        pooled_output = bert_outputs.pooler_output  # shape: (batch_size, hidden_size)
        logits = self.classifier(pooled_output)  # shape: (batch_size, num_labels)
        loss = None
        if labels is not None:
            loss_fn = nn.CrossEntropyLoss()
            loss = loss_fn(logits, labels)
        return {"loss": loss, "logits": logits, "hidden": pooled_output}

cfg.device

model = BertForSentimentAnalysis(cfg.model_name, cfg.num_labels)
model.to(cfg.device)

optimizer = AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)
total_steps = len(train_dataloader) // cfg.gradient_accumulation_steps * cfg.epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=cfg.warmup_steps, num_training_steps=total_steps)



# training
def train():
    model.train()
    global_step = 0
    scaler = None
    for epoch in range(cfg.epochs):
        print(f"Epoch {epoch + 1}/{cfg.epochs}")
        epoch_loss = 0
        progress_bar = tqdm(train_dataloader, desc="Training", leave=False)
        for step, batch in enumerate(progress_bar):
            input_id = batch['input_ids'].to(cfg.device)
            attention_mask = batch['attention_mask'].to(cfg.device)
            token_type_ids = batch['token_type_ids']
            if token_type_ids is not None:
                token_type_ids = token_type_ids.to(cfg.device)
            # labels = batch['label'].to(cfg.device)
            labels = (batch["label"] if "label" in batch else batch["labels"]).to(cfg.device)


            outputs = model(input_ids=input_id, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)
            loss = outputs["loss"]
            loss_value = loss.item()

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
            epoch_loss += loss_value
            global_step += 1

            if global_step % cfg.logging_steps == 0:
                avg = epoch_loss / global_step
                progress_bar.set_postfix({"avg_loss": avg, "lost_loss": loss_value})
        avg_epoch_loss = epoch_loss / len(train_dataloader)
        print(f"Epoch: {epoch+1} finished. Avg loss: {avg_epoch_loss}")

        eval_metrics = evaluate()
        print(f"Epoch: {epoch+1} finished. Eval metrics\n: {eval_metrics}")

        # save model
        model_to_save = model.module if hasattr(model, "module") else model
        model_to_save.save_pretrained(cfg.output_dir)
        torch.save(model_to_save.state_dict(), os.path.join(cfg.output_dir, "pytorch_model.bin"))
        tokenizer.save_pretrained(cfg.output_dir)

def predict_dataloader(dataloader: DataLoader):
    model.eval()
    all_preds = []
    all_probs = []
    all_labels = []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Predicting", leave=False):
            input_ids = batch["input_ids"].to(cfg.device)
            attention_mask = batch["attention_mask"].to(cfg.device)
            token_type_ids = batch.get("token_type_ids")
            if token_type_ids is not None:
                token_type_ids = token_type_ids.to(cfg.device)
            # labels = batch["label"].to(cfg.device)
            labels = (batch["label"] if "label" in batch else batch["labels"]).to(cfg.device)


            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
            logits = outputs["logits"]  # shape (batch, num_labels)
            probs = torch.softmax(logits, dim=-1).cpu().numpy()
            preds = np.argmax(probs, axis=-1).tolist()

            all_preds.extend(preds)
            all_probs.append(probs)
            all_labels.extend(labels.cpu().tolist())

    all_probs = np.vstack(all_probs)
    return all_labels, all_preds, all_probs

def evaluate():
    y_true, y_pred, y_probs = predict_dataloader(eval_dataloader)
    acc = accuracy_score(y_true, y_pred)
    cm = confusion_matrix(y_true, y_pred)
    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average="binary")
    # Also report macro F1 in case of class imbalance
    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average="macro")
    print("Confusion Matrix:\n", cm)
    print("Classification Report:\n", classification_report(y_true, y_pred, digits=4))
    metrics = {
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "f1_macro": f1_macro,
        "confusion_matrix": cm,
    }
    return metrics

# Train the model
train()

final_metrics = evaluate()
print("Final metrics:", final_metrics)

def predict_sentences(sentences, neutral_threshold = (0.45, 0.55)):
    model.eval()
    enc = tokenizer(sentences, truncation=True, padding=True, max_length=cfg.max_length, return_tensors="pt")
    input_ids = enc["input_ids"].to(cfg.device)
    attention_mask = enc["attention_mask"].to(cfg.device)
    token_type_ids = enc.get("token_type_ids")
    if token_type_ids is not None:
        token_type_ids = token_type_ids.to(cfg.device)
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        logits = outputs["logits"]
        probs = torch.softmax(logits, dim=-1).cpu().numpy()  # shape (n, num_labels)

    results = []
    for i, p in enumerate(probs):
        # For binary: assume label 1 is 'positive', label 0 is 'negative'
        if cfg.num_labels == 2:
            pos_prob = float(p[1])
            low, high = neutral_threshold
            if pos_prob < low:
                label = "negative"
                label_id = 0
            elif pos_prob > high:
                label = "positive"
                label_id = 1
            else:
                label = "neutral"
                label_id = None
        else:
            # Multiclass: choose max
            label_id = int(np.argmax(p))
            label = f"label_{label_id}"

        results.append({
            "text": sentences[i],
            "predicted_label": label,
            "label_id": label_id,
            "probabilities": p.tolist(),
        })
    return results

example_texts = [
        "This movie was fantastic! I loved the characters and the storyline.",
        "Terrible film. Waste of time and money.",
        "It was okay â€” some parts were good, some were boring."
    ]

preds = predict_sentences(example_texts)
for p in preds:
    print("Text:", p["text"])
    print("Predicted:", p["predicted_label"], "Probs:", p["probabilities"])
    print("-" * 60)

